[{"/Users/john/codeclan_work/final_project/full_stack_audio_processing_with_functions/audio_processing/client/src/index.js":"1","/Users/john/codeclan_work/final_project/full_stack_audio_processing_with_functions/audio_processing/client/src/App.js":"2","/Users/john/codeclan_work/final_project/full_stack_audio_processing_with_functions/audio_processing/client/src/reportWebVitals.js":"3","/Users/john/codeclan_work/final_project/full_stack_audio_processing_with_functions/audio_processing/client/src/components/AudioPlayer/tracks.js":"4","/Users/john/codeclan_work/final_project/full_stack_audio_processing_with_functions/audio_processing/client/src/components/Analysers/AudioAnalyser.js":"5","/Users/john/codeclan_work/final_project/full_stack_audio_processing_with_functions/audio_processing/client/src/components/AudioPlayer/AudioPlayer.jsx":"6","/Users/john/codeclan_work/final_project/full_stack_audio_processing_with_functions/audio_processing/client/src/components/AudioPlayer/AudioControls.jsx":"7","/Users/john/codeclan_work/final_project/full_stack_audio_processing_with_functions/audio_processing/client/src/components/Visualisers/FrequencyVisualiser.js":"8","/Users/john/codeclan_work/final_project/full_stack_audio_processing_with_functions/audio_processing/client/src/components/Visualisers/WaveformVisualiser.js":"9","/Users/john/codeclan_work/final_project/full_stack_audio_processing_with_functions/audio_processing/client/src/containers/MediaPlayer.jsx":"10","/Users/john/codeclan_work/final_project/full_stack_audio_processing_with_functions/audio_processing/client/src/components/TrackMenu/UploadForm.jsx":"11"},{"size":500,"mtime":1624800491770,"results":"12","hashOfConfig":"13"},{"size":481,"mtime":1625413373038,"results":"14","hashOfConfig":"13"},{"size":362,"mtime":1624800491772,"results":"15","hashOfConfig":"13"},{"size":708,"mtime":1625240271247,"results":"16","hashOfConfig":"13"},{"size":3096,"mtime":1625342312189,"results":"17","hashOfConfig":"13"},{"size":5175,"mtime":1625342312190,"results":"18","hashOfConfig":"13"},{"size":1912,"mtime":1625240131027,"results":"19","hashOfConfig":"13"},{"size":1805,"mtime":1625344294403,"results":"20","hashOfConfig":"13"},{"size":2006,"mtime":1625347793755,"results":"21","hashOfConfig":"13"},{"size":2272,"mtime":1625413436358,"results":"22","hashOfConfig":"13"},{"size":697,"mtime":1625415587868,"results":"23","hashOfConfig":"13"},{"filePath":"24","messages":"25","errorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"26"},"kxb1hw",{"filePath":"27","messages":"28","errorCount":0,"warningCount":2,"fixableErrorCount":0,"fixableWarningCount":0,"source":null},{"filePath":"29","messages":"30","errorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"26"},{"filePath":"31","messages":"32","errorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"26"},{"filePath":"33","messages":"34","errorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"35","usedDeprecatedRules":"26"},{"filePath":"36","messages":"37","errorCount":0,"warningCount":2,"fixableErrorCount":0,"fixableWarningCount":0,"source":"38","usedDeprecatedRules":"26"},{"filePath":"39","messages":"40","errorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"26"},{"filePath":"41","messages":"42","errorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"43","usedDeprecatedRules":"26"},{"filePath":"44","messages":"45","errorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"46","usedDeprecatedRules":"26"},{"filePath":"47","messages":"48","errorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0},{"filePath":"49","messages":"50","errorCount":2,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":null},"/Users/john/codeclan_work/final_project/full_stack_audio_processing_with_functions/audio_processing/client/src/index.js",[],["51","52"],"/Users/john/codeclan_work/final_project/full_stack_audio_processing_with_functions/audio_processing/client/src/App.js",["53","54"],"/Users/john/codeclan_work/final_project/full_stack_audio_processing_with_functions/audio_processing/client/src/reportWebVitals.js",[],"/Users/john/codeclan_work/final_project/full_stack_audio_processing_with_functions/audio_processing/client/src/components/AudioPlayer/tracks.js",[],"/Users/john/codeclan_work/final_project/full_stack_audio_processing_with_functions/audio_processing/client/src/components/Analysers/AudioAnalyser.js",["55"],"import React, {useEffect, useState, useRef} from 'react';\nimport WaveformVisualiser from '../Visualisers/WaveformVisualiser';\nimport FrequencyVisualiser from '../Visualisers/FrequencyVisualiser';\n\nconst AudioAnalyser = ({ mode, input, visualiserType }) => {\n\n    const [audioData, setAudioData] = useState(new Uint8Array(0));\n    \n    // creates an audio context and stores it in ref    \n    const audioContextRef = useRef(new (window.AudioContext || window.webkitAudioContext)());\n    // sets audioContext to be the current ref of audioContext \n    const audioContext = audioContextRef.current;\n    \n    const sourceRef = useRef(null);\n    let source = sourceRef.current;\n    const analyserRef = useRef(audioContext.createAnalyser())\n    const analyser = analyserRef.current;\n    const [analyserDisconnected, setAnalyserDisconnected] = useState(false)\n    \n    useEffect( () => {\n        console.log('analyser input', input)\n        // empty request animation frame Id\n        let rafId; \n        \n        \n        \n        // Creates a data Array which is half the length of the fftSize;\n        // it takes in unsigned integers  \n        const dataArray = new Uint8Array(analyser.frequencyBinCount);\n        // connects the audio stream to the analyser node\n\n        if(mode === \"track\"){\n            source = audioContext.createMediaElementSource(input);\n            source.connect(analyser).connect(audioContext.destination);\n        } else {\n            source = audioContext.createMediaStreamSource(input);\n            source.connect(analyser);\n        }\n        \n        const tick = () => {\n            // copies wave form data into the dataArray which is passed in as an argument   \n            analyser.getByteTimeDomainData(dataArray)\n            // sets audioData to be the value of a copy of dataArray\n            setAudioData([...dataArray])\n            // requests a re-render while calling tick in a recursive loop\n            rafId = requestAnimationFrame(tick);\n        }\n    \n        rafId = requestAnimationFrame(tick);\n\n        return function cleanup() {\n            console.log(\"disconnect analyser\")\n            if(mode === \"track\"){\n                console.log(\"mode is track\")\n                source.disconnect(analyser);\n                setAnalyserDisconnected(true)\n            } else {\n                console.log(\"mode is not track\")\n                source.disconnect()\n            }\n            cancelAnimationFrame(rafId);\n            console.log('clean up on aisle 3')   \n        }\n\n    }, [mode, input])\n\n    return(\n        <div>\n            {visualiserType === \"Waveform\" ? \n            <WaveformVisualiser \n                audioData={audioData} \n                analyserDisconnected={analyserDisconnected} \n                setAnalyserDisconnected={setAnalyserDisconnected}\n            /> :\n            <FrequencyVisualiser \n                audioData={audioData} \n                analyser={analyser}\n                setAnalyserDisconnected={setAnalyserDisconnected}\n            /> \n            }\n        </div>\n        \n    )\n\n}\n\nexport default AudioAnalyser;","/Users/john/codeclan_work/final_project/full_stack_audio_processing_with_functions/audio_processing/client/src/components/AudioPlayer/AudioPlayer.jsx",["56","57"],"import React, {useState, useEffect, useRef} from 'react';\nimport AudioControls from './AudioControls.jsx';\n\nconst AudioPlayer = ({ tracks, onChangeTrack, onPauseTrack }) => {\n\n    // index of track being played\n    const [trackIndex, setTrackIndex] = useState(0);\n    // current progress of track being played\n    const [trackProgress, setTrackProgress] = useState(0);\n    // wether or not track is being played\n    const [isPlaying, setIsPlaying] = useState(false);\n    // current title and source equal the current track index\n    const {title, audioSrc} = tracks[trackIndex];\n    \n    // stores Audio element plus audio source in a ref\n    const audioRef = useRef(new Audio(audioSrc));\n    \n    const intervalRef = useRef();\n    const isReady = useRef(false);\n\n    const { duration } = audioRef.current;\n    // const currentPercentage = duration ? `${(trackProgress / duration) * 100}%` : '0%';\n    // const trackStyling = `-webkit-gradient(linear, 0% 0%, 100% 0%, color-stop(${currentPercentage}, #fff), color-stop(${currentPercentage}, #777))`;\n\n    // PREV -  handles previous track click\n    const toPrevTrack = () => {\n        // if trackIndex minus 1 is less than zero, set track index to the last track\n        if (trackIndex - 1 < 0){\n            setTrackIndex(tracks.length - 1);\n        } else {\n            setTrackIndex(trackIndex - 1);\n        }\n    }\n    \n    // NEXT - handles next track click\n    const toNextTrack = () => {\n        // if trackIndex is less than tracks length go to next track, otherwise go to first track\n        if (trackIndex < tracks.length -1){\n            setTrackIndex(trackIndex + 1);\n        } else {\n            setTrackIndex(0);\n        }\n    }\n    \n    // PLAY\n    useEffect(() => {\n        // when isPlaying changes:\n        // if isPlaying state is false, play the track in audio tag \n        if(isPlaying) {\n            // update track for analyser\n            \n            audioRef.current.play();\n            console.log('setting track')\n            onChangeTrack(audioRef.current)\n            startTimer();\n        } else {\n            // otherwise clear the intervalRef and pause the track in audio tag\n            console.log('pause track')\n            clearInterval(intervalRef.current)\n            audioRef.current.pause();\n            // onPauseTrack();\n        }\n    },[isPlaying])\n\n    useEffect(() => {\n        // pause and clean up on unmount / clear any setInterval timers\n\n        return () => {\n            audioRef.current.pause();\n            clearInterval(intervalRef.current);\n        }\n\n    }, [])\n\n    // NEXT track\n    useEffect(() => {\n        \n        // runs when trackIndex is updated, allowing current track to be paused while\n        // updating the value of audioRef to new source, resetting the progress state and \n        // setting new track to play\n\n        audioRef.current.pause()\n        audioRef.current = new Audio(audioSrc);\n        setTrackProgress(audioRef.current.currentTime);\n\n        if (isReady.current) {\n            audioRef.current.play();\n            setIsPlaying(true);\n            startTimer()\n        } else {\n            isReady.current = true;\n        }\n    }, [trackIndex])\n\n    const startTimer = () => {\n        // clear any timers already running\n        clearInterval(intervalRef.current);\n\n        // check track every second, if ended reset the player, otherwise update track progress\n        intervalRef.current = setInterval(() => {\n            if (audioRef.current.ended) {\n                setTrackProgress(0);\n                setIsPlaying(false);\n                console.log(\"player reset:\", audioRef.current)\n            } else {\n                setTrackProgress(audioRef.current.currentTime);\n            }\n        }, [1000])\n    }\n\n\n    const onScrub = (value) => {\n        // clear any timers already running\n        clearInterval(intervalRef.current);\n        audioRef.current.currentTime = value;\n        setTrackProgress(audioRef.current.currentTime);\n    }\n\n    const onScrubEnd = () => {\n        // if isPlayings value is false then set it to true\n        if (!isPlaying) {\n            setIsPlaying(true);\n        }\n        startTimer();\n    }   \n\n    return(\n        \n        <div className=\"audio-player\">\n            <div className=\"track-info\">\n                <h3 className=\"title\">{title}</h3>\n                <AudioControls\n                    isPlaying={isPlaying}\n                    onPrevClick={toPrevTrack}\n                    onNextClick={toNextTrack}\n                    onPlayPauseClick={setIsPlaying}\n                    // onPlay={toggleTrack}\n                />\n                <input \n                    type=\"range\"\n                    value={trackProgress}\n                    step=\"1\"\n                    min=\"0\"\n                    max={duration ? duration : `${duration}`}\n                    className=\"progress\"\n                    onChange={(e) => onScrub(e.target.value)}\n                    onMouseUp={onScrubEnd}\n                    onKeyUp={onScrubEnd}\n                    // style={{ background: trackStyling}}\n                />\n            </div>\n        </div>\n\n    ) \n            \n\n}\n\nexport default AudioPlayer;","/Users/john/codeclan_work/final_project/full_stack_audio_processing_with_functions/audio_processing/client/src/components/AudioPlayer/AudioControls.jsx",[],"/Users/john/codeclan_work/final_project/full_stack_audio_processing_with_functions/audio_processing/client/src/components/Visualisers/FrequencyVisualiser.js",["58"],"import React, { useRef, useEffect } from 'react';\n\n//let testWaveFormRef = useRef(Array.from({length: 1024}, () => Math.floor(Math.random() * 255)));\n//  const testWaveForm = testWaveFormRef.current;\n\nconst FrequencyVisualiser = ({ audioData, analyser }) => {\n\n    const canvasRef = useRef();\n\n    useEffect(() => {\n\n        let canvas = canvasRef.current;\n        let height = canvas.height;\n        let width = canvas.width;\n        let context = canvas.getContext('2d');\n        let randomColour = \"#\" + ((1 << 24) * Math.random() | 0).toString(16)\n\n        // a quarter of the fft size default\n        analyser.fftSize = 256;\n        console.log(analyser)\n        //bufferLength equals half the fftSize i.e. 128\n        let bufferLength = analyser.frequencyBinCount;\n        console.log(\"buffer length\", bufferLength);\n        // let dataArray = new Uint8Array(bufferLength);\n\n        context.clearRect(0, 0, width, height);\n\n        const render = () => {\n            \n            context.fillStyle = 'rgb(0, 0 , 0)';\n            context.fillRect(0, 0, width, height);\n\n            let barWidth = (width / bufferLength) * 2.5;\n            let barHeight;\n            let x = 0;\n\n            for (var i = 0; i < bufferLength; i++) {\n                // the height of a bar equals the current audio sample value halved\n                barHeight = audioData[i] / 0.3;\n\n                context.fillStyle = randomColour;\n                context.fillRect(x, height - barHeight / 2, barWidth, barHeight)\n\n                x += barWidth + 1;\n            }\n        };\n\n        render()\n\n    }, [audioData]);\n\n    return (\n        <canvas\n            className=\"frequency-canvas\"\n            width=\"550\"\n            height=\"550\"\n            ref={canvasRef}\n        />\n    )\n\n}\n\nexport default FrequencyVisualiser;","/Users/john/codeclan_work/final_project/full_stack_audio_processing_with_functions/audio_processing/client/src/components/Visualisers/WaveformVisualiser.js",["59"],"import React, {useRef, useEffect} from 'react';\n\n//let testWaveFormRef = useRef(Array.from({length: 1024}, () => Math.floor(Math.random() * 255)));\n//  const testWaveForm = testWaveFormRef.current;\n\nconst WaveformVisualiser = ({audioData, setAnalyserDisconnected, analyserDisconnected}) => {\n\n    const canvasRef = useRef();\n    \n    //clear the canvas every 30 seconds\n    // useEffect(() => {\n        \n    //     const clearCanvas = () => {\n    //         context.clearRect(0, 0, width, height);\n    //     }\n    //     setInterval(clearCanvas, 30000)\n\n    // },[])\n\n    useEffect(() => {\n        let canvas = canvasRef.current;\n        let height = canvas.height;\n        let width = canvas.width;\n        let context = canvas.getContext('2d');\n        let x = 0;\n        let sliceWidth = width / audioData.length;\n        let randomColour = \"#\" + ((1<<24)*Math.random() | 0).toString(16)\n        context.fillRect(0, 0, canvas.width, canvas.height)\n        // if the analyser has been disconnected clear the canvas and reset analyserDisconnected to false\n        if(analyserDisconnected){\n            context.clearRect(0, 0, width, height);\n            setAnalyserDisconnected(false)\n        }\n        \n\n        const renderWaveform = () => {\n            context.lineWidth = 2;\n            context.strokeStyle = randomColour;\n            \n            context.beginPath();\n            context.moveTo(0, height / 2);\n\n            for(const item of audioData) {\n                const y = (item / 255.0) * height;\n                context.lineTo(x, y);\n                x += sliceWidth;\n            }\n            // console.log(\"Audio-data:\", audioData)\n            context.lineTo(x, height / 2);\n            context.stroke();\n            \n        }\n        \n        renderWaveform()\n        \n\n    }, [audioData])\n\n    return(\n        <canvas \n            className=\"canvas\"\n            width=\"550\" \n            height=\"550\" \n            ref={canvasRef}\n        />\n    )\n\n}\n\nexport default WaveformVisualiser;","/Users/john/codeclan_work/final_project/full_stack_audio_processing_with_functions/audio_processing/client/src/containers/MediaPlayer.jsx",[],"/Users/john/codeclan_work/final_project/full_stack_audio_processing_with_functions/audio_processing/client/src/components/TrackMenu/UploadForm.jsx",["60","61","62"],{"ruleId":"63","replacedBy":"64"},{"ruleId":"65","replacedBy":"66"},{"ruleId":"67","severity":1,"message":"68","line":2,"column":16,"nodeType":"69","messageId":"70","endLine":2,"endColumn":25},{"ruleId":"67","severity":1,"message":"71","line":2,"column":27,"nodeType":"69","messageId":"70","endLine":2,"endColumn":35},{"ruleId":"72","severity":1,"message":"73","line":33,"column":22,"nodeType":"74","endLine":33,"endColumn":66},{"ruleId":"72","severity":1,"message":"75","line":63,"column":7,"nodeType":"76","endLine":63,"endColumn":18,"suggestions":"77"},{"ruleId":"72","severity":1,"message":"78","line":93,"column":8,"nodeType":"76","endLine":93,"endColumn":20,"suggestions":"79"},{"ruleId":"72","severity":1,"message":"80","line":50,"column":8,"nodeType":"76","endLine":50,"endColumn":19,"suggestions":"81"},{"ruleId":"72","severity":1,"message":"82","line":57,"column":8,"nodeType":"76","endLine":57,"endColumn":19,"suggestions":"83"},{"ruleId":"67","severity":1,"message":"84","line":12,"column":11,"nodeType":"69","messageId":"70","endLine":12,"endColumn":19},{"ruleId":"85","severity":2,"message":"86","line":14,"column":9,"nodeType":"69","messageId":"87","endLine":14,"endColumn":19},{"ruleId":"85","severity":2,"message":"88","line":15,"column":13,"nodeType":"69","messageId":"87","endLine":15,"endColumn":22},"no-native-reassign",["89"],"no-negated-in-lhs",["90"],"no-unused-vars","'useEffect' is defined but never used.","Identifier","unusedVar","'useState' is defined but never used.","react-hooks/exhaustive-deps","Assignments to the 'source' variable from inside React Hook useEffect will be lost after each render. To preserve the value over time, store it in a useRef Hook and keep the mutable value in the '.current' property. Otherwise, you can move this variable directly inside useEffect.","CallExpression","React Hook useEffect has a missing dependency: 'onChangeTrack'. Either include it or remove the dependency array. If 'onChangeTrack' changes too often, find the parent component that defines it and wrap that definition in useCallback.","ArrayExpression",["91"],"React Hook useEffect has a missing dependency: 'audioSrc'. Either include it or remove the dependency array.",["92"],"React Hook useEffect has a missing dependency: 'analyser'. Either include it or remove the dependency array.",["93"],"React Hook useEffect has missing dependencies: 'analyserDisconnected' and 'setAnalyserDisconnected'. Either include them or remove the dependency array. If 'setAnalyserDisconnected' changes too often, find the parent component that defines it and wrap that definition in useCallback.",["94"],"'onSubmit' is assigned a value but never used.","no-undef","'postUpload' is not defined.","undef","'addUpload' is not defined.","no-global-assign","no-unsafe-negation",{"desc":"95","fix":"96"},{"desc":"97","fix":"98"},{"desc":"99","fix":"100"},{"desc":"101","fix":"102"},"Update the dependencies array to be: [isPlaying, onChangeTrack]",{"range":"103","text":"104"},"Update the dependencies array to be: [audioSrc, trackIndex]",{"range":"105","text":"106"},"Update the dependencies array to be: [analyser, audioData]",{"range":"107","text":"108"},"Update the dependencies array to be: [analyserDisconnected, audioData, setAnalyserDisconnected]",{"range":"109","text":"110"},[2329,2340],"[isPlaying, onChangeTrack]",[3167,3179],"[audioSrc, trackIndex]",[1587,1598],"[analyser, audioData]",[1798,1809],"[analyserDisconnected, audioData, setAnalyserDisconnected]"]