{"ast":null,"code":"var _jsxFileName = \"/Users/john/codeclan_work/final_project/full_stack_audio_processing_with_functions/audio_processing/src/components/AudioAnalyser.js\",\n    _s = $RefreshSig$();\n\nimport React, { useEffect, useState } from 'react';\nimport AudioVisualiser from './AudioVisualiser';\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\n\nconst AudioAnalyser = ({\n  audioInput\n}) => {\n  _s();\n\n  const [audioData, setAudioData] = useState(new Uint8Array(0));\n  useEffect(() => {\n    // sets audio data to be a Uint8Array which is half as long as the analyser fftSize:\n    // determines the amount of data values available for visualisation\n    // setDataArray(new Uint8Array(analyser.frequencyBinCount)) ;\n    // connect the audio analyser to the source of audio\n    // audioContextSource.connect(analyser);\n    // set the request animation frame Id for use when app dismounts/cancels and calls \n    // tick\n    let rafId;\n    const audioContext = new (window.AudioContext || window.webkitAudioContext)();\n    const analyser = audioContext.createAnalyser();\n    const dataArray = new Uint8Array(analyser.frequencyBinCount); // const [dataArray, setDataArray] = useState(new Uint8Array());\n    // const [audioData, setAudioData] = useState(new Uint8Array());\n\n    const source = audioContext.createMediaStreamSource(audioInput);\n    source.connect(analyser);\n\n    const tick = () => {\n      // copies wave form data into the dataArray which is passed in as an argument   \n      analyser.getByteTimeDomainData(dataArray); // setDataArray([...dataArray]);\n\n      console.log(\"audio data:\", dataArray);\n      setAudioData([...dataArray]); // set AudioData to be data contained in dataArray so it can be passed down to\n      // visualiser as a prop\n\n      rafId = requestAnimationFrame(tick);\n    };\n\n    rafId = requestAnimationFrame(tick);\n    return function cleanup() {\n      cancelAnimationFrame(rafId);\n    };\n  }, []);\n  return /*#__PURE__*/_jsxDEV(AudioVisualiser, {\n    audioData: audioData\n  }, void 0, false, {\n    fileName: _jsxFileName,\n    lineNumber: 50,\n    columnNumber: 9\n  }, this);\n};\n\n_s(AudioAnalyser, \"vp759RvJ0/M2OESOZvwirEnYLkY=\");\n\n_c = AudioAnalyser;\nexport default AudioAnalyser;\n\nvar _c;\n\n$RefreshReg$(_c, \"AudioAnalyser\");","map":{"version":3,"sources":["/Users/john/codeclan_work/final_project/full_stack_audio_processing_with_functions/audio_processing/src/components/AudioAnalyser.js"],"names":["React","useEffect","useState","AudioVisualiser","AudioAnalyser","audioInput","audioData","setAudioData","Uint8Array","rafId","audioContext","window","AudioContext","webkitAudioContext","analyser","createAnalyser","dataArray","frequencyBinCount","source","createMediaStreamSource","connect","tick","getByteTimeDomainData","console","log","requestAnimationFrame","cleanup","cancelAnimationFrame"],"mappings":";;;AAAA,OAAOA,KAAP,IAAeC,SAAf,EAA0BC,QAA1B,QAAyC,OAAzC;AACA,OAAOC,eAAP,MAA4B,mBAA5B;;;AAGC,MAAMC,aAAa,GAAG,CAAC;AAAEC,EAAAA;AAAF,CAAD,KAAoB;AAAA;;AAEvC,QAAM,CAACC,SAAD,EAAYC,YAAZ,IAA4BL,QAAQ,CAAC,IAAIM,UAAJ,CAAe,CAAf,CAAD,CAA1C;AAKAP,EAAAA,SAAS,CAAE,MAAM;AACb;AACA;AACA;AACA;AACA;AACA;AACA;AACA,QAAIQ,KAAJ;AACA,UAAMC,YAAY,GAAG,KAAKC,MAAM,CAACC,YAAP,IAAuBD,MAAM,CAACE,kBAAnC,GAArB;AACA,UAAMC,QAAQ,GAAGJ,YAAY,CAACK,cAAb,EAAjB;AACA,UAAMC,SAAS,GAAG,IAAIR,UAAJ,CAAeM,QAAQ,CAACG,iBAAxB,CAAlB,CAXa,CAYb;AACA;;AACA,UAAMC,MAAM,GAAGR,YAAY,CAACS,uBAAb,CAAqCd,UAArC,CAAf;AACAa,IAAAA,MAAM,CAACE,OAAP,CAAeN,QAAf;;AACA,UAAMO,IAAI,GAAG,MAAM;AACf;AACAP,MAAAA,QAAQ,CAACQ,qBAAT,CAA+BN,SAA/B,EAFe,CAGf;;AACAO,MAAAA,OAAO,CAACC,GAAR,CAAY,aAAZ,EAA0BR,SAA1B;AACAT,MAAAA,YAAY,CAAC,CAAC,GAAGS,SAAJ,CAAD,CAAZ,CALe,CAMf;AACA;;AACAP,MAAAA,KAAK,GAAGgB,qBAAqB,CAACJ,IAAD,CAA7B;AACH,KATD;;AAWAZ,IAAAA,KAAK,GAAGgB,qBAAqB,CAACJ,IAAD,CAA7B;AAIA,WAAO,SAASK,OAAT,GAAmB;AACtBC,MAAAA,oBAAoB,CAAClB,KAAD,CAApB;AACH,KAFD;AAIH,GAnCQ,EAmCN,EAnCM,CAAT;AAqCA,sBACI,QAAC,eAAD;AAAiB,IAAA,SAAS,EAAEH;AAA5B;AAAA;AAAA;AAAA;AAAA,UADJ;AAIH,CAhDA;;GAAMF,a;;KAAAA,a;AAkDP,eAAeA,aAAf","sourcesContent":["import React, {useEffect, useState} from 'react'\nimport AudioVisualiser from './AudioVisualiser'\n\n\n const AudioAnalyser = ({ audioInput }) => {\n\n    const [audioData, setAudioData] = useState(new Uint8Array(0))\n    \n    \n    \n    \n    useEffect( () => {\n        // sets audio data to be a Uint8Array which is half as long as the analyser fftSize:\n        // determines the amount of data values available for visualisation\n        // setDataArray(new Uint8Array(analyser.frequencyBinCount)) ;\n        // connect the audio analyser to the source of audio\n        // audioContextSource.connect(analyser);\n        // set the request animation frame Id for use when app dismounts/cancels and calls \n        // tick\n        let rafId;\n        const audioContext = new (window.AudioContext || window.webkitAudioContext)()\n        const analyser = audioContext.createAnalyser();\n        const dataArray = new Uint8Array(analyser.frequencyBinCount);\n        // const [dataArray, setDataArray] = useState(new Uint8Array());\n        // const [audioData, setAudioData] = useState(new Uint8Array());\n        const source = audioContext.createMediaStreamSource(audioInput)\n        source.connect(analyser)\n        const tick = () => {\n            // copies wave form data into the dataArray which is passed in as an argument   \n            analyser.getByteTimeDomainData(dataArray)\n            // setDataArray([...dataArray]);\n            console.log(\"audio data:\",dataArray)\n            setAudioData([...dataArray])\n            // set AudioData to be data contained in dataArray so it can be passed down to\n            // visualiser as a prop\n            rafId = requestAnimationFrame(tick);\n        }\n    \n        rafId = requestAnimationFrame(tick);\n\n        \n\n        return function cleanup() {\n            cancelAnimationFrame(rafId);\n        }\n\n    }, [])\n\n    return(\n        <AudioVisualiser audioData={audioData}/>\n    )\n\n}\n\nexport default AudioAnalyser;"]},"metadata":{},"sourceType":"module"}