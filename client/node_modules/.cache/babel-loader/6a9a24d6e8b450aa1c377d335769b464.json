{"ast":null,"code":"var _jsxFileName = \"/Users/john/codeclan_work/final_project/full_stack_audio_processing_with_functions/audio_processing/src/components/AudioAnalyser.js\",\n    _s = $RefreshSig$();\n\nimport React, { useEffect, useState } from 'react';\nimport AudioVisualiser from './AudioVisualiser';\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\n\nconst AudioAnalyser = ({\n  audioContextSource\n}) => {\n  _s();\n\n  const audioContext = new (window.AudioContext || window.webkitAudioContext)();\n  const analyser = audioContext.createAnalyser();\n  const dataArray = new Uint8Array(analyser.frequencyBinCount); // const [dataArray, setDataArray] = useState(new Uint8Array());\n  // const [audioData, setAudioData] = useState(new Uint8Array());\n\n  const source = audioContext.createMediaStreamSource(audioContextSource);\n  useEffect(() => {\n    // sets audio data to be a Uint8Array which is half as long as the analyser fftSize:\n    // determines the amount of data values available for visualisation\n    setDataArray(new Uint8Array(analyser.frequencyBinCount)); // connect the audio analyser to the source of audio\n\n    audioContextSource.connect(analyser); // set the request animation frame Id for use when app dismounts/cancels and calls \n    // tick\n\n    requestAnimationFrame(tick);\n  }, []);\n\n  const tick = () => {\n    // copies wave form data into the dataArray which is passed in as an argument   \n    analyser.getByteTimeDomainData(dataArray); // setDataArray([...dataArray]);\n\n    console.log(\"audio data:\", dataArray); // set AudioData to be data contained in dataArray so it can be passed down to\n    // visualiser as a prop\n\n    requestAnimationFrame(tick);\n  }; // function componentWillUnmount() {\n  //     cancelAnimationFrame(rafId);\n  //     analyser.disconnect();\n  //     source.disconnect();\n  // }\n\n\n  return /*#__PURE__*/_jsxDEV(AudioVisualiser, {\n    dataArray: dataArray\n  }, void 0, false, {\n    fileName: _jsxFileName,\n    lineNumber: 48,\n    columnNumber: 9\n  }, this) // <p>Testing broken shit</p>\n  ;\n};\n\n_s(AudioAnalyser, \"OD7bBpZva5O2jO+Puf00hKivP7c=\");\n\n_c = AudioAnalyser;\nexport default AudioAnalyser;\n\nvar _c;\n\n$RefreshReg$(_c, \"AudioAnalyser\");","map":{"version":3,"sources":["/Users/john/codeclan_work/final_project/full_stack_audio_processing_with_functions/audio_processing/src/components/AudioAnalyser.js"],"names":["React","useEffect","useState","AudioVisualiser","AudioAnalyser","audioContextSource","audioContext","window","AudioContext","webkitAudioContext","analyser","createAnalyser","dataArray","Uint8Array","frequencyBinCount","source","createMediaStreamSource","setDataArray","connect","requestAnimationFrame","tick","getByteTimeDomainData","console","log"],"mappings":";;;AAAA,OAAOA,KAAP,IAAeC,SAAf,EAA0BC,QAA1B,QAAyC,OAAzC;AACA,OAAOC,eAAP,MAA4B,mBAA5B;;;AAGC,MAAMC,aAAa,GAAG,CAAC;AAAEC,EAAAA;AAAF,CAAD,KAA4B;AAAA;;AAE/C,QAAMC,YAAY,GAAG,KAAKC,MAAM,CAACC,YAAP,IAAuBD,MAAM,CAACE,kBAAnC,GAArB;AACA,QAAMC,QAAQ,GAAGJ,YAAY,CAACK,cAAb,EAAjB;AACA,QAAMC,SAAS,GAAG,IAAIC,UAAJ,CAAeH,QAAQ,CAACI,iBAAxB,CAAlB,CAJ+C,CAK/C;AACA;;AACA,QAAMC,MAAM,GAAGT,YAAY,CAACU,uBAAb,CAAqCX,kBAArC,CAAf;AAKAJ,EAAAA,SAAS,CAAE,MAAM;AACb;AACA;AACAgB,IAAAA,YAAY,CAAC,IAAIJ,UAAJ,CAAeH,QAAQ,CAACI,iBAAxB,CAAD,CAAZ,CAHa,CAIb;;AACAT,IAAAA,kBAAkB,CAACa,OAAnB,CAA2BR,QAA3B,EALa,CAMb;AACA;;AACAS,IAAAA,qBAAqB,CAACC,IAAD,CAArB;AAEH,GAVQ,EAUN,EAVM,CAAT;;AAaA,QAAMA,IAAI,GAAG,MAAM;AACf;AACAV,IAAAA,QAAQ,CAACW,qBAAT,CAA+BT,SAA/B,EAFe,CAGf;;AACAU,IAAAA,OAAO,CAACC,GAAR,CAAY,aAAZ,EAA0BX,SAA1B,EAJe,CAKf;AACA;;AACAO,IAAAA,qBAAqB,CAACC,IAAD,CAArB;AACH,GARD,CAzB+C,CAoC/C;AACA;AACA;AACA;AACA;;;AAEA,sBACI,QAAC,eAAD;AAAiB,IAAA,SAAS,EAAER;AAA5B;AAAA;AAAA;AAAA;AAAA,UADJ,CAEI;AAFJ;AAKH,CA/CA;;GAAMR,a;;KAAAA,a;AAiDP,eAAeA,aAAf","sourcesContent":["import React, {useEffect, useState} from 'react'\nimport AudioVisualiser from './AudioVisualiser'\n\n\n const AudioAnalyser = ({ audioContextSource }) => {\n\n    const audioContext = new (window.AudioContext || window.webkitAudioContext)()\n    const analyser = audioContext.createAnalyser();\n    const dataArray = new Uint8Array(analyser.frequencyBinCount);\n    // const [dataArray, setDataArray] = useState(new Uint8Array());\n    // const [audioData, setAudioData] = useState(new Uint8Array());\n    const source = audioContext.createMediaStreamSource(audioContextSource)\n\n    \n    \n    \n    useEffect( () => {\n        // sets audio data to be a Uint8Array which is half as long as the analyser fftSize:\n        // determines the amount of data values available for visualisation\n        setDataArray(new Uint8Array(analyser.frequencyBinCount)) ;\n        // connect the audio analyser to the source of audio\n        audioContextSource.connect(analyser);\n        // set the request animation frame Id for use when app dismounts/cancels and calls \n        // tick\n        requestAnimationFrame(tick);\n\n    }, [])\n\n    \n    const tick = () => {\n        // copies wave form data into the dataArray which is passed in as an argument   \n        analyser.getByteTimeDomainData(dataArray)\n        // setDataArray([...dataArray]);\n        console.log(\"audio data:\",dataArray)\n        // set AudioData to be data contained in dataArray so it can be passed down to\n        // visualiser as a prop\n        requestAnimationFrame(tick);\n    }\n\n\n    // function componentWillUnmount() {\n    //     cancelAnimationFrame(rafId);\n    //     analyser.disconnect();\n    //     source.disconnect();\n    // }\n\n    return(\n        <AudioVisualiser dataArray={dataArray}/>\n        // <p>Testing broken shit</p>\n    )\n\n}\n\nexport default AudioAnalyser;"]},"metadata":{},"sourceType":"module"}